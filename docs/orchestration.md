# aaiclick Orchestration Backend Specification

## Overview

The aaiclick orchestration backend enables distributed execution of data processing workflows across multiple workers. It manages job scheduling, task distribution, and execution coordination using PostgreSQL as the state store.

### Motivation

As aaiclick scales to handle large-scale data processing, we need:
- **Distributed execution**: Parallelize work across multiple workers
- **Dynamic task generation**: Create new tasks during execution (e.g., via `map()` operations)
- **Reliable state management**: Track job progress with crash recovery
- **Ordered execution**: Preserve temporal causality via creation timestamps

## Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    Context Manager                      │
│  ┌──────────────────────┐  ┌──────────────────────────┐│
│  │  ClickHouse Client   │  │  PostgreSQL Session      ││
│  │  (Data operations)   │  │  (Orchestration state)   ││
│  └──────────────────────┘  └──────────────────────────┘│
└─────────────────────────────────────────────────────────┘
           │                              │
           │ Objects/Views                │ Jobs/Tasks/Groups
           ▼                              ▼
┌────────────────────┐      ┌──────────────────────────────┐
│   ClickHouse DB    │      │      PostgreSQL Database     │
│   (Object data)    │      │  ┌────┐ ┌──────┐ ┌────────┐ │
└────────────────────┘      │  │Jobs│─│Tasks │ │Workers │ │
                            │  └────┘ └──────┘ └────────┘ │
                            │  ┌──────┐ ┌──────────────┐  │
                            │  │Groups│ │Dependencies  │  │
                            │  └──────┘ └──────────────┘  │
                            └──────────────────────────────┘
                                        ▲
                                        │ Polls and executes
                                        │
                            ┌───────────┴────────────────┐
                            │ Worker Pool (N processes)  │
                            └────────────────────────────┘
```

**Context Dual-Database Architecture**:
- **ClickHouse Client**: Manages Object data (tables, views, queries)
- **PostgreSQL Session**: Manages orchestration state (jobs, tasks, groups, dependencies)
- Context provides unified interface to both databases
- `context.create_object()` → ClickHouse
- `context.apply(tasks)` → PostgreSQL

**Worker Architecture**: Each worker is a single process that can execute multiple tasks concurrently using async/await. This allows efficient utilization of I/O-bound operations (database queries, network calls) without blocking.

## Technology Stack

- **SQLModel**: Type-safe ORM with Pydantic integration
- **Alembic**: Database migrations for PostgreSQL
- **PostgreSQL**: Orchestration state store (jobs, tasks, groups, dependencies)
- **ClickHouse**: Data storage (Objects and Views)
- **asyncpg** (via SQLModel): Async PostgreSQL driver

### Why Dual-Database Architecture?

**PostgreSQL for Orchestration State:**
- **ACID compliance**: Strong consistency guarantees for job state
- **Row-level locking**: Safe concurrent task claiming by workers (`FOR UPDATE SKIP LOCKED`)
- **JSONB support**: Flexible task parameter storage
- **Mature ecosystem**: Alembic migrations, connection pooling, monitoring
- **Relational integrity**: Foreign keys for job→task→group relationships

**ClickHouse for Data:**
- **Columnar storage**: Efficient for analytical workloads
- **High performance**: Fast aggregations and scans
- **Scalability**: Handles massive datasets
- **aaiclick Objects**: All data processing operates on ClickHouse tables

**Context bridges both databases**: Provides unified API for orchestration (PostgreSQL) and data operations (ClickHouse).

**High-Level Factory APIs**:
- **`create_task(callback, kwargs)`**: Factory for creating Task objects from callback strings
  - Generates 63-bit snowflake ID for task
- **`create_job(name, entry)`**: Factory for creating Job with single entry point (Task or callback)
  - Generates 63-bit snowflake ID for job
- **`context.apply(tasks)`**: Commits DAG (tasks, groups, dependencies) to PostgreSQL
  - Generates 63-bit snowflake IDs for groups
- Factories provide simple interface for common workflows
- IDs generated before database insertion (no round-trip needed)

## Data Models

### ID Generation Strategy

**All entities use 63-bit Snowflake IDs** generated by the aaiclick framework (not database auto-increment):

- **Job IDs**: Framework-generated snowflake IDs (63-bit positive integers)
- **Task IDs**: Framework-generated snowflake IDs (63-bit positive integers)
- **Group IDs**: Framework-generated snowflake IDs (63-bit positive integers)

**Why Snowflake IDs?**
- **Distributed generation**: IDs can be generated independently across processes without coordination
- **Time-ordered**: IDs encode creation timestamp for temporal ordering
- **No database round-trip**: IDs assigned before database insert
- **Consistency with aaiclick Objects**: Same ID strategy as ClickHouse data tables

**Database Storage**:
- PostgreSQL type: `BIGINT` (signed int64, range: -2^63 to 2^63-1)
- Python/SQLModel type: `int` (unbounded)
- **Important**: PostgreSQL has no native unsigned uint64 type
- **Solution**: Use BIGINT (signed int64) with positive-only snowflake IDs
- Snowflake IDs must use ≤ 63 bits (max value: 2^63-1 = 9,223,372,036,854,775,807)
- Sign bit remains 0 for all generated IDs
- Database enforces uniqueness constraints via primary keys but does not generate IDs

**Snowflake ID Generation Requirements**:
- Generate IDs in range [0, 2^63-1]
- Use 63 bits: timestamp (41 bits) + machine_id (10 bits) + sequence (12 bits)
- Same constraint as ClickHouse Object IDs
- Framework must ensure IDs never exceed int64 max positive value

**SQLModel Field Definition**:
```python
from sqlmodel import Field, Column
from sqlalchemy import BigInteger

# Correct: ID generated by framework, stored as BIGINT (signed int64)
id: int = Field(sa_column=Column(BigInteger, primary_key=True, autoincrement=False))

# Simplified (SQLModel infers BigInteger from int type):
id: int = Field(primary_key=True)

# Wrong: Optional with default=None triggers auto-increment
id: Optional[int] = Field(default=None, primary_key=True)
```

### Status Enums

```python
from enum import StrEnum

class JobStatus(StrEnum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class TaskStatus(StrEnum):
    PENDING = "pending"
    CLAIMED = "claimed"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class WorkerStatus(StrEnum):
    ACTIVE = "active"
    IDLE = "idle"
    STOPPED = "stopped"
```

### Job

Represents a workflow containing one or more tasks.

```python
from datetime import datetime
from typing import Optional
from sqlmodel import Field, SQLModel, Relationship

class Job(SQLModel, table=True):
    __tablename__ = "jobs"

    id: int = Field(primary_key=True)
    # uint64 snowflake ID generated by framework (not database auto-increment)
    # Database enforces uniqueness constraint

    # Job metadata
    name: str = Field(index=True)
    description: Optional[str] = None

    # Status tracking
    status: JobStatus = Field(default=JobStatus.PENDING, index=True)

    # Timestamps
    created_at: datetime = Field(default_factory=datetime.utcnow, index=True)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None

    # Relationships
    tasks: list["Task"] = Relationship(back_populates="job")
    groups: list["Group"] = Relationship(back_populates="job")
```

**Job Status Lifecycle:**
```
PENDING → RUNNING → COMPLETED
                  → FAILED
                  → CANCELLED
```

### Group

Represents a logical grouping of tasks and other groups within a job. Groups can be nested.

```python
from datetime import datetime
from typing import Optional
from sqlmodel import Field, SQLModel, Relationship

class Group(SQLModel, table=True):
    __tablename__ = "groups"

    id: int = Field(primary_key=True)
    # uint64 snowflake ID generated by framework
    job_id: int = Field(foreign_key="jobs.id", index=True)
    parent_group_id: Optional[int] = Field(default=None, foreign_key="groups.id", index=True)

    # Group metadata
    name: str = Field(index=True)
    description: Optional[str] = None

    # Timestamps
    created_at: datetime = Field(default_factory=datetime.utcnow, index=True)

    # Relationships
    job: Job = Relationship(back_populates="groups")
    tasks: list["Task"] = Relationship(back_populates="group")
    parent_group: Optional["Group"] = Relationship(
        back_populates="child_groups",
        sa_relationship_kwargs={"remote_side": "Group.id"}
    )
    child_groups: list["Group"] = Relationship(back_populates="parent_group")
```

**Nested Group Support:**
- Groups can contain tasks via `group_id` foreign key in Task
- Groups can contain other groups via `parent_group_id` foreign key
- Enables hierarchical workflow organization
- Dependencies can be defined at any level of the hierarchy

### Dependency

Unified dependency table supporting all dependency types between tasks and groups.

```python
from sqlmodel import Field, SQLModel

class Dependency(SQLModel, table=True):
    __tablename__ = "dependencies"

    # Entity that must complete first
    previous_id: int = Field(primary_key=True, index=True)
    previous_type: str = Field(primary_key=True)  # 'task' or 'group'

    # Entity that waits (executes after previous completes)
    next_id: int = Field(primary_key=True, index=True)
    next_type: str = Field(primary_key=True)  # 'task' or 'group'
```

**Supported Dependency Types:**
- Task → Task: `previous_type='task'`, `next_type='task'`
- Task → Group: `previous_type='task'`, `next_type='group'` (all tasks in next group wait for previous task)
- Group → Task: `previous_type='group'`, `next_type='task'` (next task waits for all tasks in previous group)
- Group → Group: `previous_type='group'`, `next_type='group'` (next group waits for all tasks in previous group)

**Benefits:**
- Single unified table for all dependency relationships
- Intuitive previous/next naming for workflow dependencies
- Flexible schema supporting future entity types
- Simplified querying across all dependency types
- Validation enforced at ORM/application level

### Task

Represents a single executable unit of work within a job.

```python
from datetime import datetime
from typing import Optional, Dict, Any
from sqlmodel import Field, SQLModel, Relationship, Column, JSON

class Task(SQLModel, table=True):
    __tablename__ = "tasks"

    id: int = Field(primary_key=True)
    # uint64 snowflake ID generated by framework
    job_id: int = Field(foreign_key="jobs.id", index=True)
    group_id: Optional[int] = Field(default=None, foreign_key="groups.id", index=True)

    # Execution specification
    entrypoint: str = Field()
    # Format: "module.submodule.function" (importable callable)
    # Example: "aaiclick.operators.map_function"

    kwargs: Dict[str, Any] = Field(default_factory=dict, sa_column=Column(JSON))
    # Dictionary mapping parameter names to serialized values
    # See "Task Parameter Serialization" section for object_type formats

    # Status tracking
    status: TaskStatus = Field(default=TaskStatus.PENDING, index=True)

    # Result
    result_table_id: Optional[str] = None
    # ClickHouse table ID of the result Object

    # Logging
    log_path: Optional[str] = None
    # Path to task log file: {get_logs_dir()}/{task_id}.log
    # Captures stdout and stderr during task execution
    # OS-dependent defaults: ~/.aaiclick/logs (macOS), /var/log/aaiclick (Linux)

    # Error tracking
    error_message: Optional[str] = None
    retry_count: int = Field(default=0)
    max_retries: int = Field(default=3)

    # Worker assignment
    worker_id: Optional[str] = Field(default=None, index=True)
    # Identifier of the worker executing this task

    # Timestamps
    created_at: datetime = Field(default_factory=datetime.utcnow, index=True)
    claimed_at: Optional[datetime] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None

    # Relationships
    job: Job = Relationship(back_populates="tasks")
    group: Optional[Group] = Relationship(back_populates="tasks")
```

**Task Dependencies and Groups**:
- Each task belongs to one Group (optional, via group_id foreign key)
- Groups can be nested (group contains other groups via parent_group_id)
- All dependencies (task → task, task → group, group → task, group → group) managed via unified Dependency table
- A task can only be claimed if all its direct dependencies are satisfied (completed tasks/groups)

**Task Status Lifecycle:**
```
PENDING → CLAIMED → RUNNING → COMPLETED
                            → FAILED → PENDING (if retries remain)
                                    → FAILED (max retries exceeded)
        → CANCELLED
```

**Dependency Constraints**:
- A task can only be claimed if all its `previous` dependencies (tasks/groups) have status COMPLETED
- Circular dependencies are not allowed
- Dependencies managed via unified Dependency table with previous/next relationships

### Worker

Represents an active worker process.

```python
from datetime import datetime
from typing import Optional
from sqlmodel import Field, SQLModel

class Worker(SQLModel, table=True):
    __tablename__ = "workers"

    id: str = Field(primary_key=True)
    # Format: "{hostname}:{pid}:{timestamp}"

    # Worker metadata
    hostname: str = Field(index=True)
    pid: int

    # Status
    status: WorkerStatus = Field(default=WorkerStatus.ACTIVE, index=True)

    # Heartbeat
    last_heartbeat: datetime = Field(default_factory=datetime.utcnow, index=True)

    # Statistics
    tasks_completed: int = Field(default=0)
    tasks_failed: int = Field(default=0)

    # Timestamps
    started_at: datetime = Field(default_factory=datetime.utcnow)
```

## Task Parameter Serialization

Task kwargs are stored as JSONB and can contain three types of parameters identified by `object_type`:

### Object Parameters

Reference to a full aaiclick Object (entire ClickHouse table):

```json
{
    "object_type": "object",
    "table_id": "t123456789"
}
```

Worker deserializes to an `Object` instance with the specified table.

### View Parameters

Reference to a subset/view of an Object with query constraints:

```json
{
    "object_type": "view",
    "table_id": "t123456789",
    "offset": 0,
    "limit": 10000,
    "where": "value > 100"
}
```

Worker deserializes to a `View` instance. All constraint fields are optional except `table_id`. Default ordering is `aai_id ASC`.

### Python Object Parameters

Native Python values (JSONB serializable):

```json
{
    "object_type": "pyobj",
    "value": {"threshold": 0.5, "max_iter": 100}
}
```

Worker deserializes to the native Python value. Supports primitives, lists, dicts, etc.

### Example Task Kwargs

```python
# Task with mixed parameter types
task_kwargs = {
    "input_data": {
        "object_type": "view",
        "table_id": "t987654321",
        "offset": 10000,
        "limit": 10000
    },
    "config": {
        "object_type": "pyobj",
        "value": {"threshold": 0.5}
    },
    "reference_table": {
        "object_type": "object",
        "table_id": "t111222333"
    }
}
```

## Task Execution Flow

### 1. Job Creation

```python
from aaiclick.orchestration import create_job, create_task

# Option 1: Create job with callback string
job = await create_job(
    name="data_processing_pipeline",
    entry="myapp.processors.load_and_process_data"
)

# Option 2: Create task first, then job
task = create_task(
    callback="myapp.processors.load_and_process_data",
    kwargs={
        "source": {
            "object_type": "pyobj",
            "value": "dataset_v1"
        }
    }
)
job = await create_job(
    name="data_processing_pipeline",
    entry=task
)
```

### 2. Dynamic Task Creation

Tasks can create additional tasks during execution via aaiclick operators:

```python
# In aaiclick/operators.py
async def map(callback: str, obj: Object) -> Object:
    """
    Apply callback to each element of obj in parallel.
    Creates one task per chunk of data using offset/limit.
    """
    from aaiclick.orchestration import create_task, get_current_context

    # Get current context (has access to job_id and PostgreSQL session)
    ctx = get_current_context()

    # Get total row count without reading data
    total_rows = await obj.count()
    chunk_size = 10000  # Configurable chunk size

    # Create task for each chunk using create_task factory
    tasks = []
    for offset in range(0, total_rows, chunk_size):
        task = create_task(
            callback=callback,
            kwargs={
                "chunk": {
                    "object_type": "view",
                    "table_id": obj.table_id,
                    "offset": offset,
                    "limit": chunk_size
                }
            }
        )
        tasks.append(task)

    # Commit all tasks to database (context automatically sets job_id)
    await ctx.apply(tasks)

    # Return handle to future results
    num_chunks = (total_rows + chunk_size - 1) // chunk_size
    return await create_result_collector(ctx.job_id, num_chunks)
```

### 3. Worker Task Execution Loop

```python
# Worker main loop
async def worker_main_loop(worker_id: str):
    while True:
        # Claim next available task (atomic operation)
        # Prioritizes tasks from oldest running jobs
        task = await claim_next_task(worker_id)

        if task is None:
            await asyncio.sleep(1)  # No tasks available
            continue

        try:
            # Update task status
            await update_task_status(task.id, "running")

            # Set up task logging
            log_path = f"{get_logs_dir()}/{task.id}.log"
            await update_task_log_path(task.id, log_path)

            # Execute task with Context bound to job
            async with Context(job_id=task.job_id) as ctx:
                # Capture stdout/stderr to log file
                with capture_task_output(task.id):
                    # Import and execute entrypoint
                    func = import_function(task.entrypoint)

                    # Deserialize task kwargs (see Task Parameter Serialization section)
                    # Converts object_type formats to Object/View/native Python instances
                    task_kwargs = deserialize_task_params(task.kwargs, ctx)

                    # All print() and errors write to {get_logs_dir()}/{task.id}.log
                    result_obj = await func(**task_kwargs)

            # Store result
            await update_task_result(
                task.id,
                result_table_id=result_obj.table_id,
                status=TaskStatus.COMPLETED
            )

        except Exception as e:
            # Handle failure (error also logged to task log file)
            await handle_task_failure(task, error=str(e))
```

### 4. Task Claiming (Atomic)

Uses PostgreSQL row-level locking for safe concurrent access:

```sql
-- Implemented via SQLModel/SQLAlchemy
-- Prioritizes tasks from oldest running jobs first
-- Respects all dependency types via unified dependency table
-- Also marks job as started and running when first task is claimed
WITH claimed_task AS (
    UPDATE tasks
    SET
        status = 'claimed',
        worker_id = :worker_id,
        claimed_at = NOW()
    WHERE id = (
        SELECT t.id FROM tasks t
        JOIN jobs j ON t.job_id = j.id
        WHERE t.status = 'pending'
        -- Check previous task → next task dependencies
        AND NOT EXISTS (
            SELECT 1 FROM dependencies d
            JOIN tasks prev ON d.previous_id = prev.id
            WHERE d.next_id = t.id
            AND d.next_type = 'task'
            AND d.previous_type = 'task'
            AND prev.status != 'completed'
        )
        -- Check previous group → next task dependencies (all tasks in previous group must be completed)
        AND NOT EXISTS (
            SELECT 1 FROM dependencies d
            JOIN tasks prev ON d.previous_id = prev.group_id
            WHERE d.next_id = t.id
            AND d.next_type = 'task'
            AND d.previous_type = 'group'
            AND prev.status != 'completed'
        )
        -- Check previous task → next group dependencies (if task is in next group that waits for previous task)
        AND NOT EXISTS (
            SELECT 1 FROM dependencies d
            JOIN tasks prev ON d.previous_id = prev.id
            WHERE d.next_id = t.group_id
            AND d.next_type = 'group'
            AND d.previous_type = 'task'
            AND prev.status != 'completed'
            AND t.group_id IS NOT NULL
        )
        -- Check previous group → next group dependencies (if task is in next group that waits for previous group)
        AND NOT EXISTS (
            SELECT 1 FROM dependencies d
            JOIN tasks prev ON d.previous_id = prev.group_id
            WHERE d.next_id = t.group_id
            AND d.next_type = 'group'
            AND d.previous_type = 'group'
            AND prev.status != 'completed'
            AND t.group_id IS NOT NULL
        )
        ORDER BY j.started_at ASC
        LIMIT 1
        FOR UPDATE SKIP LOCKED
    )
    RETURNING *
)
UPDATE jobs
SET
    started_at = COALESCE(started_at, NOW()),
    status = CASE WHEN started_at IS NULL THEN 'running' ELSE status END
WHERE id = (SELECT job_id FROM claimed_task)
RETURNING (SELECT * FROM claimed_task);
```

**Key features:**
- `FOR UPDATE SKIP LOCKED`: Skip rows locked by other workers
- `NOT EXISTS`: Only claim tasks where all previous dependencies (tasks/groups) are completed
- `ORDER BY j.started_at ASC`: Prioritize tasks from oldest running jobs
- `COALESCE(started_at, NOW())`: Atomically set job's started_at when first task is claimed
- `CASE WHEN started_at IS NULL`: Set job status to 'running' on first task claim
- Atomic update: Prevents race conditions
- Unified dependency checking: Single table handles all dependency types (task→task, task→group, group→task, group→group)

## API / Interfaces

### Job Management

```python
from typing import Callable, Union
from aaiclick.orchestration import (
    create_job,
    create_task,
    get_job,
    list_jobs,
    cancel_job,
    Task
)

# Create task from callback function
task = create_task(
    callback="myapp.processors.process_data",
    kwargs={
        "input": {
            "object_type": "object",
            "table_id": "tbl_xyz"
        }
    }
)

# Create job with single entry point task
job = await create_job(
    name="data_pipeline",
    entry=task  # Single Task as entry point
)

# Or create job with callback directly
job = await create_job(
    name="data_pipeline",
    entry="myapp.processors.process_data"  # Callback string as entry point
)

# Get job status
job = await get_job(job_id)
print(f"Status: {job.status}, Progress: {job.completed_tasks}/{job.total_tasks}")

# List jobs
jobs = await list_jobs(status=JobStatus.RUNNING)

# Cancel job
await cancel_job(job_id)

# Test job execution (synchronous, for testing/debugging)
job.test()  # Similar to Airflow - invokes worker execute flow
```

**`job.test()` Method:**
```python
def test(self):
    """
    Execute job synchronously in current process (test mode).

    Invokes the worker execute flow for testing and debugging.
    Similar to Airflow's test execution mode - runs tasks sequentially
    in the current process instead of distributing to workers.

    Useful for:
    - Local development and debugging
    - Integration tests
    - Validating job logic before deployment

    Note: Blocks until job completes (COMPLETED/FAILED status)
    """
```

**`create_task()` Factory:**
```python
def create_task(callback: str, kwargs: dict) -> Task:
    """
    Factory function for creating Task objects.

    Args:
        callback: Importable function path (e.g., "myapp.module.function")
        kwargs: Task parameters (supports object/view/pyobj serialization)

    Returns:
        Task object (not yet committed to database)
    """
```

**`create_job()` Factory:**
```python
async def create_job(
    name: str,
    entry: Union[str, Task]
) -> Job:
    """
    Create a new job with a single entry point.

    Args:
        name: Job name
        entry: Entry point - either callback string or Task object

    Returns:
        Job object with entry task committed to database
    """
```

### Task Management

```python
from aaiclick.orchestration import (
    add_task_to_job,
    get_task,
    retry_failed_tasks
)

# Add task to existing job
await add_task_to_job(
    job_id=job.id,
    entrypoint="myapp.process",
    kwargs={"data": "tbl_xyz"}
)

# Get task details
task = await get_task(task_id)

# Retry failed tasks
await retry_failed_tasks(job_id)
```

### Context API for DAG Construction

```python
from aaiclick.orchestration import Context, Task, Group

# Create orchestration context for a job
context = Context(job_id=job.id)

# Define tasks and groups in memory
task1 = Task(entrypoint="myapp.func1", kwargs={...})
task2 = Task(entrypoint="myapp.func2", kwargs={...})
group1 = Group(name="processing")

# Set up dependencies using operators
task1 >> task2
task2.group_id = group1.id

# Commit tasks, groups, and dependencies to database
await context.apply(task1)  # Apply single task
await context.apply([task1, task2, group1])  # Apply multiple tasks/groups

# context.apply() performs:
# - Inserts/updates Task and Group records in database
# - Inserts Dependency records created by >> and << operators
# - Validates circular dependencies
# - Returns committed objects with IDs assigned
```

**`context.apply()` Signature:**
```python
async def apply(
    self,
    items: Union[Task, Group, List[Union[Task, Group]]]
) -> Union[Task, Group, List[Union[Task, Group]]]:
    """
    Commit tasks, groups, and their dependencies to the database.

    Args:
        items: Single Task/Group or list of Task/Group objects

    Returns:
        Same items with database IDs populated

    Raises:
        CircularDependencyError: If circular dependencies detected
    """
```

### DAG Construction with Dependency Operators

Airflow-like syntax for defining dependencies between tasks and groups:

```python
from aaiclick.orchestration import Task, Group

# Task → Task dependencies
task1 = Task(entrypoint="myapp.extract", kwargs={...})
task2 = Task(entrypoint="myapp.transform", kwargs={...})
task3 = Task(entrypoint="myapp.load", kwargs={...})

# task2 depends on task1 (task1 executes before task2)
task1 >> task2

# task3 depends on task2
task2 >> task3

# Equivalent chaining
task1 >> task2 >> task3

# Reverse syntax (task1 depends on task2)
task2 << task1  # Same as: task1 >> task2

# Group → Group dependencies
extract_group = Group(name="extract")
transform_group = Group(name="transform")
load_group = Group(name="load")

extract_group >> transform_group >> load_group

# Task → Group dependencies (task completes before all tasks in group start)
validation_task = Task(entrypoint="myapp.validate", kwargs={...})
validation_task >> transform_group

# Group → Task dependencies (all tasks in group complete before task starts)
transform_group >> final_report_task

# Mixed dependencies
task1 >> group1 >> task2 >> group2 >> task3

# Multiple dependencies (fan-out and fan-in)
# Fan-out: task1 must complete before task2, task3, and task4 can start
task1 >> [task2, task3, task4]

# Fan-in: task5 waits for task2, task3, and task4 to complete
[task2, task3, task4] >> task5

# Complex DAG
source_task >> extract_group >> [transform_task1, transform_task2]
[transform_task1, transform_task2] >> load_group >> final_task

# Commit all tasks, groups, and dependencies to the database
await context.apply([source_task, extract_group, transform_task1, transform_task2, load_group, final_task])
# Or pass individual items
await context.apply(source_task)
await context.apply(extract_group)
```

**Using `context.apply()` to commit DAGs:**

```python
from aaiclick.orchestration import Context, Task, Group

# Create context for a job
context = Context(job_id=job.id)

# Define tasks
extract = Task(entrypoint="myapp.extract", kwargs={...})
transform = Task(entrypoint="myapp.transform", kwargs={...})
load = Task(entrypoint="myapp.load", kwargs={...})

# Define dependencies
extract >> transform >> load

# Commit all tasks and dependencies to database
await context.apply([extract, transform, load])
# context.apply() saves:
# - All Task/Group objects
# - All Dependency records created by >> and << operators
# - Validates no circular dependencies exist
```

**Operator Semantics:**
- `A >> B`: B depends on A (A is previous, B is next)
- `A << B`: A depends on B (B is previous, A is next)
- `A >> [B, C, D]`: B, C, and D all depend on A (fan-out)
- `[A, B, C] >> D`: D depends on all of A, B, and C (fan-in)
- Works with any combination of Task and Group objects
- Dependencies are stored in the unified Dependency table
- Circular dependencies are detected and rejected

**How Fan-In Works (`[A, B] >> C`):**

Fan-in is **not** a built-in Python feature. It uses Python's **reverse operator** mechanism:

1. Python evaluates `[A, B] >> C`
2. Python first tries `list.__rshift__([A, B], C)` - but lists don't support `>>`
3. Python falls back to the **reverse operator**: `C.__rrshift__([A, B])`
4. The `__rrshift__` method on `C` receives the list `[A, B]` and calls `C.depends_on(A)` and `C.depends_on(B)`

This means:
- `__rshift__`: Normal operator called on left operand (`A >> B` → `A.__rshift__(B)`)
- `__rrshift__`: Reverse operator called on right operand when left doesn't support operation (`[A, B] >> C` → `C.__rrshift__([A, B])`)
- Same pattern for `__lshift__` and `__rlshift__`

**Implementation:**
```python
class Task(SQLModel, table=True):
    # ... fields ...

    def depends_on(self, other: Union["Task", "Group"]) -> "Task":
        """
        Declare that this task depends on another task or group.
        Creates a Dependency record in the database.

        Args:
            other: Task or Group that must complete before this task

        Returns:
            self (for chaining)
        """
        dependency = Dependency(
            previous_id=other.id,
            previous_type="task" if isinstance(other, Task) else "group",
            next_id=self.id,
            next_type="task"
        )
        session.add(dependency)
        return self

    def __rshift__(self, other: Union["Task", "Group", list]) -> Union["Task", "Group", list]:
        """A >> B: B depends on A"""
        if isinstance(other, list):
            for item in other:
                item.depends_on(self)
            return other
        else:
            other.depends_on(self)
            return other

    def __lshift__(self, other: Union["Task", "Group", list]) -> "Task":
        """A << B: A depends on B"""
        if isinstance(other, list):
            for item in other:
                self.depends_on(item)
        else:
            self.depends_on(other)
        return self

    def __rrshift__(self, other: Union["Task", "Group", list]) -> "Task":
        """Reverse: [A, B] >> C means C depends on A and B (fan-in)"""
        if isinstance(other, list):
            for item in other:
                self.depends_on(item)
        else:
            self.depends_on(other)
        return self

    def __rlshift__(self, other: Union["Task", "Group", list]) -> Union["Task", "Group", list]:
        """Reverse: [A, B] << C means A and B depend on C (fan-out)"""
        if isinstance(other, list):
            for item in other:
                item.depends_on(self)
            return other
        else:
            other.depends_on(self)
            return other

class Group(SQLModel, table=True):
    # ... fields ...

    def depends_on(self, other: Union[Task, "Group"]) -> "Group":
        """
        Declare that this group depends on a task or another group.
        Creates a Dependency record in the database.

        Args:
            other: Task or Group that must complete before tasks in this group

        Returns:
            self (for chaining)
        """
        dependency = Dependency(
            previous_id=other.id,
            previous_type="task" if isinstance(other, Task) else "group",
            next_id=self.id,
            next_type="group"
        )
        session.add(dependency)
        return self

    def __rshift__(self, other: Union[Task, "Group", list]) -> Union[Task, "Group", list]:
        """A >> B: B depends on A"""
        if isinstance(other, list):
            for item in other:
                item.depends_on(self)
            return other
        else:
            other.depends_on(self)
            return other

    def __lshift__(self, other: Union[Task, "Group", list]) -> "Group":
        """A << B: A depends on B"""
        if isinstance(other, list):
            for item in other:
                self.depends_on(item)
        else:
            self.depends_on(other)
        return self

    def __rrshift__(self, other: Union[Task, "Group", list]) -> "Group":
        """Reverse: [A, B] >> C means C depends on A and B (fan-in)"""
        if isinstance(other, list):
            for item in other:
                self.depends_on(item)
        else:
            self.depends_on(other)
        return self

    def __rlshift__(self, other: Union[Task, "Group", list]) -> Union[Task, "Group", list]:
        """Reverse: [A, B] << C means A and B depend on C (fan-out)"""
        if isinstance(other, list):
            for item in other:
                item.depends_on(self)
            return other
        else:
            other.depends_on(self)
            return other
```

### Worker Management

```python
from aaiclick.orchestration import (
    register_worker,
    worker_heartbeat,
    deregister_worker,
    list_workers
)

# Register worker
worker = await register_worker(hostname="worker-01", pid=12345)

# Heartbeat (periodic)
await worker_heartbeat(worker.id)

# Deregister
await deregister_worker(worker.id)

# List active workers
workers = await list_workers(status=WorkerStatus.ACTIVE)
```

## Configuration

### Environment Variables

```bash
# PostgreSQL connection
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=aaiclick
POSTGRES_PASSWORD=secret
POSTGRES_DB=aaiclick_orchestration

# Task logging (optional - defaults via get_logs_dir())
AAICLICK_LOG_DIR=<custom-path>  # Override default log directory

# Worker settings
WORKER_HEARTBEAT_INTERVAL=30  # seconds
WORKER_TASK_TIMEOUT=3600      # seconds
WORKER_MAX_RETRIES=3

# Job settings
JOB_DEFAULT_TIMEOUT=86400     # seconds (24 hours)
```

**Log Directory Resolution**:

The log directory is resolved via `get_logs_dir()` which provides OS-dependent defaults:

```python
def get_logs_dir() -> str:
    """
    Get task log directory with OS-dependent defaults.

    Defaults:
    - macOS: ${HOME}/.aaiclick/logs
    - Linux: /var/log/aaiclick

    Returns:
        Log directory path (creates if doesn't exist)
    """
    if custom_dir := os.getenv("AAICLICK_LOG_DIR"):
        return custom_dir

    if sys.platform == "darwin":  # macOS
        return os.path.expanduser("~/.aaiclick/logs")
    else:  # Linux
        return "/var/log/aaiclick"
```

**Notes**:
- For distributed workers, use a shared mount (NFS, EFS, etc.) and set `AAICLICK_LOG_DIR` to the mount path
- Single-machine deployments can use local filesystem with defaults
- Directory is created automatically if it doesn't exist

### Database Connection

```python
from sqlmodel import create_engine
from sqlalchemy.ext.asyncio import create_async_engine

DATABASE_URL = "postgresql+asyncpg://{user}:{password}@{host}:{port}/{db}"

engine = create_async_engine(
    DATABASE_URL,
    echo=False,
    pool_size=20,
    max_overflow=40
)
```

## Implementation Plan

### Phase 1: Core Infrastructure
1. Set up SQLModel models (Job, Task, Worker)
2. Configure Alembic migrations
3. Implement database connection management
4. Create basic CRUD operations

### Phase 2: Worker Implementation
1. Implement task claiming logic (with locking)
2. Build worker main loop
3. Add heartbeat mechanism
4. Implement error handling and retries

### Phase 3: Job Management
1. Job creation API
2. Task scheduling logic
3. Job status tracking
4. Progress monitoring

### Phase 4: Dynamic Task Creation
1. Add context tracking for current job
2. Implement `add_task_to_current_job()`
3. Integrate with aaiclick operators (`map`, `filter`, etc.)
4. Handle result collection

### Phase 5: Integration & Testing
1. Integrate with Context
2. Add comprehensive tests
3. Performance benchmarking
4. Documentation

## Monitoring & Observability

### Metrics to Track

```python
# Job metrics
- jobs_created_total
- jobs_completed_total
- jobs_failed_total
- job_duration_seconds

# Task metrics
- tasks_created_total
- tasks_completed_total
- tasks_failed_total
- task_duration_seconds
- task_queue_depth

# Worker metrics
- workers_active
- worker_task_execution_time
- worker_heartbeat_age
```

### Health Checks

```python
async def health_check():
    """Check orchestration backend health."""
    checks = {
        "database": await check_database_connection(),
        "workers": await check_active_workers(),
        "stale_tasks": await check_stale_tasks(),
    }
    return all(checks.values())
```

## Error Handling

### Task Failures

1. **Transient errors**: Retry up to `max_retries`
2. **Permanent errors**: Mark task as failed, fail parent job
3. **Timeout**: Kill worker, retry task on another worker

### Worker Failures

1. **Heartbeat timeout**: Mark worker as stopped
2. **Orphaned tasks**: Reclaim tasks from stopped workers
3. **Recovery**: Reset orphaned tasks to pending status

### Job Failures

1. **Task failure**: Propagate to job if critical
2. **Cancellation**: Cancel all pending tasks
3. **Cleanup**: Remove temporary resources

## Packaging Consideration

Database migrations must be bundled with the aaiclick package and executable via CLI:

```bash
python -m aaiclick migrate
```

This ensures users can initialize and upgrade the orchestration database schema without manual migration management.

## References

- [SQLModel Documentation](https://sqlmodel.tiangolo.com/)
- [Alembic Documentation](https://alembic.sqlalchemy.org/)
- [PostgreSQL Locking](https://www.postgresql.org/docs/current/explicit-locking.html)
- [aaiclick Architecture](./aaiclick.md)
